<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><title>Dmytro Yelchaninov</title><meta property="og:title" content="Dmytro Yelchaninov"><meta property="og:description" content="My portfolio and blog"><meta property="og:image" content="https://data-science.me/static/images/me.JPG"><meta property="og:url" content="https://data-science.me"><meta property="og:type" content="website"><link rel="stylesheet" href="css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"><link rel="stylesheet" href="css/hot_dog.css"><link rel="stylesheet" href="css/description_hot.css"></head><body><div class="wrapper"><header class="header"><div class="container"><div class="header__body"><a class="header__logo" href="game.html"><img id="logo" src="" alt="Logo"></a><div class="header__burger"><span class="span1"></span><span class="span2"></span><span class="span3"></span></div><nav class="header__menu"><ul class="header__list"><li><a class="header__link" href="about.html">Me</a></li><li><a class="header__link" href="projects.html">Projects</a><ul class="projects__list"><li><a class="header__link" href="hot_dog.html">Hot Dog?</a></li><li><a class="header__link" href="lasik.html">LASIK Surgery</a></li><li><a class="header__link" href="gun_violence.html">Gun Violence</a></li><li><a class="header__link" href="house.html">Amex Housing</a></li></ul></li><li><a id="game_link" href="game.html">DARTs!</a></li><li><a class="header__link" href="photo.html">Photography</a></li><li><a class="header__link" href="contact.html">Contact</a></li></ul><div class="header__social_mobile"><a href="https://linkedin.com" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com" target="_blank"><i class="fab fa-github"></i></a></div></nav><div class="header__social"><a href="https://linkedin.com/in/dmytro-yelchaninov" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com/dmytroyelchaninov" target="_blank"><i class="fab fa-github"></i></a></div></div></div></header><div class="content"><div class="container"><main> <div class="hot__wrapper"><div class="hot__description"><div class="hot__description_title"><h1 id="hot__title">What's that? <br></h1></div><div class="hot__description_text"><p id="hot__text"><p id="drop__title">UPLOAD THE IMAGE TO CHECK IF IT'S A HOT DOG! ðŸŒ­ <br></p></p><p id="hot__text">My app uses a state-of-the-art Vision Transformer (ViT) model, which is more powerful than traditional Convolutional Neural Networks (CNNs) 
like VGG or MobileNet. ViT analyzes the entire image at once, picking up global patterns and details that CNNs often miss. <br>
Thanks to its global attention mechanism, ViT hits a higher accuracy, outperforming CNNs in most cases. <br>
Can your picture break my model? <br><br>
By the way, welcome to my<a id="git-link" href="https://github.com/dmytroyelchaninov/is_hot_dog"> Github<br></a>and scroll down for more details!</p></div><div class="hot__description_arrow"><buttom id="hot__arrow">&#10094;</buttom></div></div><div class="hot__images"><div class="hot__images_dragdrop"><img id="hot__dragdrop" src="images/dragdrop.png" alt="Drag and Drop"></div><div class="hot__images_hotdog"><img id="hot__hotdog" src="images/hot_title.png" alt="Hot Dog"><form id="upload-form" method="post" enctype="multipart/form-data"></form><input id="file-input" type="file" name="file" style="display:none;" accept="image/*"></div><div class="hot__images_uploaded-image"><img id="hot__uploaded-image" src="" alt="Uploaded Image"><form id="upload-form" method="post" enctype="multipart/form-data"></form><input id="file-input" type="file" name="file" style="display:none;" accept="image/*"></div></div><div class="hot__result"><div class="hot__result_processing"><span id="rotating-dash"> </span><p id="processing_text">Processing...</p></div><div class="hot__result_text"><p id="hot__result-pos">Nice hot dog! ðŸŒ­</p><p id="hot__result-neg">That's definetely not a hot dog! ðŸš«</p></div></div><div class="hot__notification-process"><p>Hold on, image is processing</p></div><div class="hot__notification-error"><p>Oops, something went wrong</p></div></div><div class="desc__describe_container"><div class="desc__describe_wrapper"><div class="desc__describe_title"><h1 id="desc__describe_title">How does it work?</h1></div><div class="desc__describe_main"><div class="desc__describe_main_text"><p>When thinking about how large language models (LLMs) work, I came up with an idea. LLMs learn on sequences of words, which are split into tokens. The transformer architecture in LLMs allows the model to learn large-scale dependencies across those tokens, no matter their position in the sequence. This is quite different from how CNNs (Convolutional Neural Networks) work, as CNNs focus on extracting local features (edges, textures) using convolutional filters that process small regions of the image at a time.</p><p>But what if we used transformers for images? This could allow us to capture more general patterns beyond local features, and potentially achieve higher accuracy, especially in image classification problems. Of course, using transformers for images would require more memory and computational resources. However, if our goal is accuracy rather than real-time predictions, this might be worth it.</p><p>Initially, I considered flattening the entire image into a sequence. But the problem with that is the number of tokens would be too large. For example, with a 256x256 RGB image (with 3 color channels), flattening it would result in 256 * 256 * 3 = 196,608 tokens. Transformers have a time complexity of O(nÂ²), where n is the sequence length. This means processing such long sequences would be extremely computationally expensive. A more practical approach would be to split the image into smaller patches (for example, 16x16 pixels) and treat each patch as a token.</p><p>And, of course, almost any interesting idea you think of has already been implemented in some form. Enter the Vision Transformer (ViT), which leverages the same principles I just mentioned.</p><div class="desc__vision_transformer_section"><h2>Vision Transformer (ViT): How It Works</h2><ol id="desc__vit_steps"><li><b>Image Patches</b>. Instead of processing each pixel individually, ViT divides the image into fixed-size patches (e.g., 16x16 pixels). Each patch is treated as a token.</li><li><b>Tokenization</b>. The patches are flattened and treated as input tokens for the transformer.</li><li><b>Linear Projection</b>. Each patch is linearly projected into a fixed-dimensional embedding space.</li><li><b>Positional Encoding</b>. Since transformers donâ€™t have a natural sense of the position of tokens (unlike CNNs that process images spatially), ViT adds positional encodings to the patch embeddings. This allows the model to learn where each patch belongs in the image.</li><li></b>Transformer Layers</b>. These patch embeddings with positional encodings are passed through a series of transformer layers, where self-attention captures global dependencies across all patches.</li><li><b>Classification Head</b>. A special classification token is appended to the input sequence, and its final output is used for the classification decision.</li></ol></div><div class="desc__application_section"><h2>Application: Hotdog Classification</h2><p>I recently applied this idea to a simple binary classification taskâ€”whether an image contains a hotdog or not. I participated in a short hackathon where the goal was to build the most accurate model for this task. The dataset consisted of small (32x32x3) images, and the problem was surprisingly difficultâ€”some images were so unclear that even I found it hard to tell if they contained a hotdog.</p><p>Using CNN-based models (including pre-trained models), I was able to achieve around 83-85% accuracy on the test data. Even with high-end GPUs, that was the best I could do. This was a perfect example of a task with a simple goal but complex data.</p><p>When I switched to ViT, the model immediately surpassed 90% accuracy on the validation data. After about one hour of training, it achieved 97% accuracy on the test data, and I stopped the process even though the validation loss was still decreasing!</p></div><div class="desc__summary_section"><h2>Summary</h2><p>Models based on transformers, like ViT, are absolutely viable for image classification tasks, but they come with trade-offs. They require more computational resources, both for training and inference, compared to CNNs. However, if high accuracy is your priority, especially for classification tasks, using a pre-trained ViT model can be a highly effective approach. The key is to balance the computational cost against the accuracy requirements of your project.</p></div><div class="desc__additional_clarifications"><h2>Some Notes:</h2><ol><li><b>Linear Projection</b>. This step transforms each image patch into a fixed-dimensional embedding (similar to how words are embedded in NLP tasks). It doesnâ€™t directly map onto axes like mathematical projections but is a learned transformation through a weight matrix.</li><li><b>Positional Encodings</b>. These help transformers handle spatial data like images, as they ensure the model understands where each patch is located in the original image.</li><li><b>ViTâ€™s Advantage</b>. The use of self-attention allows ViT to capture global patterns in an image better than CNNs, which often focus on local features. However, transformersâ€™ quadratic complexity means they are more resource-intensive.</li></ol></div></div><div class="desc__describe_main_img"><p id="desc__describe_img_text">Image generated by DALLÂ·E based on the description above.</p><img id="desc__describe_img" src="images/hot_desc.jpg" alt="Vision Transformer"></div></div></div></div></main></div></div></div><script src="https://code.jquery.com/jquery-3.6.4.min.js"></script><script src="js/burger.js"></script><script src="js/logo.js"></script><script src="js/transition.js"></script><script src="js/hot_dog.js"></script><footer class="footer"><div class="footer__body"><div class="footer__text"><p>&copy; 2024 Yelchaninov</p></div></div></footer></body></html>