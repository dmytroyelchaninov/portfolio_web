<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><title>Dmytro Yelchaninov</title><meta property="og:title" content="Dmytro Yelchaninov"><meta property="og:description" content="My portfolio and blog"><meta property="og:image" content="https://data-science.me/static/images/me.JPG"><meta property="og:url" content="https://data-science.me"><meta property="og:type" content="website"><link rel="stylesheet" href="css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"><link rel="stylesheet" href="css/everything.css"><link rel="stylesheet" href="css/desc_everything.css"></head><body><div class="wrapper"><header class="header"><div class="container"><div class="header__body"><a class="header__logo" href="game.html"><img id="logo" src="" alt="Logo"></a><div class="header__burger"><span class="span1"></span><span class="span2"></span><span class="span3"></span></div><nav class="header__menu"><ul class="header__list"><li><a class="header__link" href="about.html">Me</a></li><li><a class="header__link" href="projects.html">Projects</a><ul class="projects__list"><li><a class="header__link" href="hot_dog.html">Hot Dog?</a></li><li><a class="header__link" href="lasik.html">LASIK Surgery</a></li><li><a class="header__link" href="gun_violence.html">Gun Violence</a></li><li><a class="header__link" href="house.html">Amex Housing</a></li><li><a class="header__link" href="everything.html">Everything</a></li></ul></li><li><a id="game_link" href="game.html">DARTs!</a></li><li><a class="header__link" href="photo.html">Photography</a></li><li><a class="header__link" href="contact.html">Contact</a></li></ul><div class="header__social_mobile"><a href="https://linkedin.com" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com" target="_blank"><i class="fab fa-github"></i></a></div></nav><div class="header__social"><a href="https://linkedin.com/in/dmytro-yelchaninov" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com/dmytroyelchaninov" target="_blank"><i class="fab fa-github"></i></a></div></div></div></header><div class="content"><div class="container"><main> <div class="desc__container"><div class="desc__wrapper"><div class="desc__title"><div id="desc__title">Everything, or “ImageGameNet”</div></div><div class="desc__main"><div class="desc__main_text"><div id="desc__header">I’m working on a model that doesn’t just recognize objects in an image but can break them down into precise details. Think of it like a game you’ve probably played before: where one person imagines a place or object, and the other player has to guess it by asking yes or no questions. You start broad: “Is it in the USA?” “Is it a big city?” Then, as you get more answers, you narrow it down: “Is it on the East Coast?” “Is it New York?” “Is it in Manhattan?” Every question eliminates possibilities until you get the final answer. Until you got that famous bakery on the Union Square.<br>Now, imagine doing this with an image. The goal is to identify everything in the picture by asking the right questions, step by step, until I know exactly what’s there in detail. This project is my attempt to recreate that experience—automatically, with precision.<br>I don’t have either the willingness to spend half of my life labeling tons of images or the computational resources to train that massive model.<br>Traditional object detection models are powerful but come with significant downsides. Training one large neural network with thousands of classes requires massive datasets, carefully labeled examples, and immense computational power. I want to avoid these challenges while still achieving accurate, detailed object detection.<br>Instead of one massive model, I’m developing a system where each neural network answers a simple yes/no question about the image. These networks work together, like a group of players solving the guessing game, to detect objects and even fine details, one step at a time.</div><div class="desc__step1"><p id="desc__step_title">Why This Approach?</p><br><p id="desc__step_text"></p>There are a few reasons I’m opting for this more modular approach:<ul><li>Simpler Datasets: Each neural network answers a binary question, which means I don’t need enormous datasets or intensive labeling. I can train each node with smaller, manageable datasets—just organize “True” and “False” examples into folders, and I’m ready to go.</li><li>Precision: Because each neural network focuses on a specific question, the system becomes more precise. I don’t need to make the model generalize across thousands of classes.</li><li>Scalability: As my project grows, I can add more questions (nodes) to the system without needing to retrain one giant model. This keeps the system flexible and expandable over time.</li></ul></div><div class="desc__step2"><p id="desc__step_title">Main Steps</p><br><p id="desc__step_text"></p>Feature Extraction:<br>The process begins by passing the image through a pretrained CNN like VGG or ResNet. Instead of having the CNN directly classify objects, I use it to extract abstract features such as shapes, edges, and textures. These features form the feature vector, which serves as the input for the navigation model, which will trigger some number of initial nodes.<br>Graph-Based Navigation Model:<br>Think of the navigation process like a graph traversal problem. Each node in this graph represents a binary classifier, answering yes/no questions about the image. The edges between the nodes represent transitions based on the answers provided by previous nodes.<br>The initial CNN could also estimate the number of distinct objects in the image, which helps guide the system in triggering the right number of nodes in parallel. Each of these nodes explores different possibilities in the image—asking more abstract questions like, “Is there an animal-shaped object?” or “Is there a man-made object?” rather than trying to define the actual class of the object.<br>This graph-based structure allows for parallel node triggering. Multiple nodes can be processed simultaneously, each representing a different branch of possible objects. The key is to dynamically adjust which nodes are triggered next, based on the answers received from previous nodes. Instead of following a fixed sequence of decisions, the system adapts the path based on the current information, just like how a Random Forest works, but with a much more flexible structure. Random Forest, actually, once trained, always follows the same tree. My goal is to have a number of “independent” nodes in each layer, so they could be connected in many different ways.<br>Dynamic Trajectory Adjustment:<br>As the system gathers answers from the nodes, it dynamically recalculates the shortest path through the decision graph. This is where graph theory really comes into play. The goal is to find the minimum number of questions (or nodes) needed to accurately identify the object. Each edge in the graph has a weight based on the complexity or importance of the question, and as answers are provided, the system recalculates the most efficient path forward, much like <a id="href_to_wiki" href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm">Dijkstra’s shortest path algorithm. <br></a><br>By adjusting the weights of the edges and pruning irrelevant branches based on Yes/No answers, the system can efficiently navigate through the possible decision paths and reach conclusions faster.<br>Object Detection and Bounding Box Generation:<br>Once enough information is gathered, the system confirms the specific object through specialized nodes (e.g., “Is it a lion?” “Is it a car?”). For each object identified, a bounding box is generated using techniques like Grad-CAM or object localization methods, marking the location of the object in the image.<br>Fine-Grained Analysis:<br>After isolating an object, the system moves into fine-grained analysis. Another Random Forest-like model (focused on precision) is used to ask specific questions about the object’s details. For instance, if the object is a person, the system might ask, “What is the person wearing?” or “Does the person have any accessories?” This part of the model works more sequentially, drilling down into details based on the bounding box region.<br>Generating the Output:<br>Finally, I pass the detected objects and their details to a language model that turns the information into a readable description. For example: <br>“A male adult tiger standing near the edge of a large body of water (like a lake or river). The surrounding area is natural ground with grass and a few scattered trees. There are also some other plants like shrubs around. No buildings, roads, or vehicles are in the picture.”</div><div class="desc__summary"><p id="desc__step_title">Trade-Offs and Why It’s Worth It:</p><br><p id="desc__step_text"></p>Nothing comes without trade-offs. A system like this, with its multiple steps and many nodes, is likely to be slower than one big neural network model. However, there are ways to improve speed by parallelizing the process, so high-end GPUs can help manage this.<br>But the benefits outweigh the downsides for me:<ul><li>No Need for Huge Datasets: With this approach, I avoid the need for massive, labeled datasets. I can train each neural network separately with much smaller datasets.</li><li>Scalability: As my project grows, I can easily add more nodes to handle new objects or features without needing to retrain an enormous model. This allows me to scale the system in a flexible way.</li><li>Navigation Model for Efficient Questioning: This likely doesn’t need a huge dataset either. It’s trained to minimize the number of questions to get to a final decision. I can take random images, label the objects I see, and use this data to teach the Random Forest to navigate the abstract questions efficiently.</li></ul></div><div class="desc__notes"><p id="desc__step_title">Final Thoughts</p><br><p id="desc__step_text"></p>This project is all about finding a balance between precision, scalability, and efficiency. By breaking down the task of object detection into smaller, modular components, I avoid the pitfalls of huge dataset requirements and complex retraining, while still achieving detailed results. And yes, it may be a bit slower—but with the right hardware, it’s a trade-off I’m willing to make for the flexibility and accuracy this approach provides.<br>In terms of performance, each individual True/False classifier is expected to be faster than a large neural network with thousands of classes. With parallel computing, I can query multiple classifiers at once, minimizing any delays. In the best-case scenario, the system will only be marginally slower than a massive model, but it offers far more precision and flexibility. Even without extreme computational power, the modular design ensures that adding new object categories won’t require retraining a gigantic model. The potential trade-off in speed is worth the precision and adaptability that this approach provides.</div></div><div class="desc__img"><p id="desc__img_text"> "A male adult tiger standing near the edge of a large body of water (like a lake or river). The surrounding area is natural ground with grass and a few scattered trees. There are also some other plants like shrubs around. No buildings, roads, or vehicles are in the picture" <br>By DALL·E.</p><img id="desc__img" src="images/everything.jpg" alt="Vision Transformer"></div></div></div></div></main></div></div></div><script src="https://code.jquery.com/jquery-3.6.4.min.js"></script><script src="js/burger.js"></script><script src="js/logo.js"></script><script src="js/transition.js"></script><footer class="footer"><div class="footer__body"><div class="footer__text"><p>&copy; 2024 Yelchaninov</p></div></div></footer></body></html>