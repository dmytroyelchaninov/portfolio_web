<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><title>Dmytro Yelchaninov</title><meta property="og:title" content="Dmytro Yelchaninov"><meta property="og:description" content="My portfolio and blog"><meta property="og:image" content="https://data-science.me/static/images/me.JPG"><meta property="og:url" content="https://data-science.me"><meta property="og:type" content="website"><link rel="stylesheet" href="css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"><link rel="stylesheet" href="css/game.css"><link rel="stylesheet" href="css/description_darts.css"></head><body><div class="wrapper"><header class="header"><div class="container"><div class="header__body"><a class="header__logo" href="game.html"><img id="logo" src="" alt="Logo"></a><div class="header__burger"><span class="span1"></span><span class="span2"></span><span class="span3"></span></div><nav class="header__menu"><ul class="header__list"><li><a class="header__link" href="about.html">Me</a></li><li><a class="header__link" href="projects.html">Projects</a><ul class="projects__list"><li><a class="header__link" href="hot_dog.html">Hot Dog?</a></li><li><a class="header__link" href="lasik.html">LASIK Surgery</a></li><li><a class="header__link" href="gun_violence.html">Gun Violence</a></li><li><a class="header__link" href="house.html">Amex Housing</a></li></ul></li><li><a id="game_link" href="game.html">DARTs!</a></li><li><a class="header__link" href="photo.html">Photography</a></li><li><a class="header__link" href="contact.html">Contact</a></li></ul><div class="header__social_mobile"><a href="https://linkedin.com" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com" target="_blank"><i class="fab fa-github"></i></a></div></nav><div class="header__social"><a href="https://linkedin.com/in/dmytro-yelchaninov" target="_blank"><i class="fab fa-linkedin"></i></a><a href="https://github.com/dmytroyelchaninov" target="_blank"><i class="fab fa-github"></i></a></div></div></div></header><div class="content"><div class="container"><main> <main><div class="game-container"><div class="explanation"><div class="explanation__title"><h1 id="explanation__title">What's going on here?! ðŸŽ¯</h1></div><div class="explanation__text"><p id="explanation__description"> Upload a picture of your dartboard, and the app will automatically detect and 
track dart positions using advanced image processing with OpenCV, YOLO and clustering methods. <br>
If you click on the TEST button, a random image from the test library will be uploaded. <br>
May be little slow, due to low-end the CPU on the server and iterative clustering, <br>
but soon it will become much faster. <br><br>
You can check the full app on my <a id="git-link" href="https://github.com/dmytroyelchaninov/dart_game_ai">Github. <br></a>Additional feautures like automatic Airdrop for Apple users, game logic and other functionalities are there.<br><br>
Scroll down for more details!</p></div><div class="explanation__arrow"><buttom id="explanation__arrow">&#10094;</buttom></div></div><div class="crowns"><img id="crowns" src="images/crowns.jpg" alt="Crowns"></div><div class="image-wrapper" id="image-wrapper"><img class="game-image" id="dartboard" src="images/title.png" alt="Dartboard"><form id="upload-form" method="post" enctype="multipart/form-data"></form><input id="file-input" type="file" name="file" style="display:none;" accept="image/*"></div><!-- Carousel container (hidden initially)--><div class="carousel" id="carousel" style="display: none;"><div class="carousel-images"><img class="carousel-image" id="carousel-processed" src="" alt="Processed Image"><img class="carousel-image" id="carousel-initial" src="" alt="Initial Image"><form id="upload-form-carousel" method="post" enctype="multipart/form-data"></form><input id="file-input-carousel" type="file" name="file" style="display:none;" accept="image/*"></div></div><div class="test-button"><img id="test-button" src="images/test_me.jpg" alt="Test Me" onclick="testImage()"></div><!-- Move the carousel control buttons outside the carousel container--><button class="carousel-control prev" onclick="prevSlide()">&#10094;</button><button class="carousel-control next" onclick="nextSlide()">&#10095;</button></div><div class="notification-process"><p>Hold on, image is processing</p></div><div class="notification-error"><p>Oops, something went wrong</p></div></main><div class="desc__describe_container"><div class="desc__describe_wrapper"><div class="desc__describe_title"><h1 id="desc__describe_title">How does it work?</h1></div><div class="desc__describe_main"><div class="desc__describe_main_text"><p> </p>This app is designed to automate the process of detecting dart hits on a dartboard and accurately calculating the score, 
overcoming the challenges of perspective distortion and varied angles. <br>
Hereâ€™s a breakdown of the core functionality:<div class="desc__vision_transformer_section"><h2>1. Initial Dart Detection with YOLO Models</h2><p>The first step in the process involves detecting the number of darts on the board and their approximate locations:</p><ol id="desc__vit_steps"><li><b>Image Patches</b>. Instead of processing each pixel individually, ViT divides the image into fixed-size patches (e.g., 16x16 pixels). Each patch is treated as a token.</li><li><b>Tokenization</b>. The patches are flattened and treated as input tokens for the transformer.</li><li><b>Linear Projection</b>. Each patch is linearly projected into a fixed-dimensional embedding space.</li><li><b>Positional Encoding</b>. Since transformers donâ€™t have a natural sense of the position of tokens (unlike CNNs that process images spatially), ViT adds positional encodings to the patch embeddings. This allows the model to learn where each patch belongs in the image.</li><li></b>Transformer Layers</b>. These patch embeddings with positional encodings are passed through a series of transformer layers, where self-attention captures global dependencies across all patches.</li><li><b>Classification Head</b>. A special classification token is appended to the input sequence, and its final output is used for the classification decision.</li></ol></div><div class="desc__application_section"><h2>Application: Hotdog Classification</h2><p>I recently applied this idea to a simple binary classification taskâ€”whether an image contains a hotdog or not. I participated in a short hackathon where the goal was to build the most accurate model for this task. The dataset consisted of small (32x32x3) images, and the problem was surprisingly difficultâ€”some images were so unclear that even I found it hard to tell if they contained a hotdog.</p><p>Using CNN-based models (including pre-trained models), I was able to achieve around 83-85% accuracy on the test data. Even with high-end GPUs, that was the best I could do. This was a perfect example of a task with a simple goal but complex data.</p><p>When I switched to ViT, the model immediately surpassed 90% accuracy on the validation data. After about one hour of training, it achieved 97% accuracy on the test data, and I stopped the process even though the validation loss was still decreasing!</p></div><div class="desc__summary_section"><h2>Summary</h2><p>Models based on transformers, like ViT, are absolutely viable for image classification tasks, but they come with trade-offs. They require more computational resources, both for training and inference, compared to CNNs. However, if high accuracy is your priority, especially for classification tasks, using a pre-trained ViT model can be a highly effective approach. The key is to balance the computational cost against the accuracy requirements of your project.</p></div><div class="desc__additional_clarifications"><h2>Some Notes:</h2><ol><li><b>Linear Projection</b>. This step transforms each image patch into a fixed-dimensional embedding (similar to how words are embedded in NLP tasks). It doesnâ€™t directly map onto axes like mathematical projections but is a learned transformation through a weight matrix.</li><li><b>Positional Encodings</b>. These help transformers handle spatial data like images, as they ensure the model understands where each patch is located in the original image.</li><li><b>ViTâ€™s Advantage</b>. The use of self-attention allows ViT to capture global patterns in an image better than CNNs, which often focus on local features. However, transformersâ€™ quadratic complexity means they are more resource-intensive.</li></ol></div></div><div class="desc__describe_main_img"><p id="desc__describe_img_text">Image generated by DALLÂ·E based on the description above.</p><img id="desc__describe_img" src="images/hot_desc.jpg" alt="Vision Transformer"></div></div></div></div></main></div></div></div><script src="https://code.jquery.com/jquery-3.6.4.min.js"></script><script src="js/burger.js"></script><script src="js/logo.js"></script><script src="js/transition.js"></script><script src="js/game.js"></script><footer class="footer"><div class="footer__body"><div class="footer__text"><p>&copy; 2024 Yelchaninov</p></div></div></footer></body></html>